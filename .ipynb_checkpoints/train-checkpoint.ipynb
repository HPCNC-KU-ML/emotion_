{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, absolute_import\n",
    "import numpy as np\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected, flatten\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d, avg_pool_2d\n",
    "from tflearn.layers.merge_ops import merge\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "from os.path import isfile, join\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prevents appearance of tensorflow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# prevents opencl usage and unnecessary logging messages\n",
    "cv2.ocl.setUseOpenCL(False)\n",
    "\n",
    "target_classes = ['angry','happy', 'neutral', 'sad', 'scared']\n",
    "# target_classes = ['sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_sets():\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "\n",
    "    for emotion in target_classes:\n",
    "        training = get_files(emotion)\n",
    "        for item in training:\n",
    "            arr = []\n",
    "            image = cv2.imread(item)  # open image\n",
    "            newimg = format_image(image)\n",
    "            \n",
    "            if(len(newimg)==1):\n",
    "                continue\n",
    "            #print(newimg.shape)\n",
    "            training_data.append(newimg)\n",
    "            arr.append(target_classes.index(emotion))\n",
    "#             training_labels.append(target_classes.index(emotion))\n",
    "            training_labels.append(arr)\n",
    "\n",
    "\n",
    "      \n",
    "\n",
    "    return training_data, training_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_files(emotion):\n",
    "    print(\"./images/%s/*\" % emotion)\n",
    "    files = glob.glob(\"./images/%s/*\" % emotion)\n",
    "    print(len(files))\n",
    "\n",
    "    random.shuffle(files)\n",
    "\n",
    "    training = files[:int(len(files))]\n",
    "#     training = files[:]\n",
    " \n",
    "\n",
    "    return training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_image(image):\n",
    "    \"\"\"\n",
    "    Function to format frame\n",
    "    \"\"\"\n",
    "    if len(image.shape) > 2 and image.shape[2] == 3:\n",
    "            # determine whether the image is color\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        # Image read from buffer\n",
    "        image = cv2.imdecode(image, cv2.CV_LOAD_IMAGE_GRAYSCALE)\n",
    "\n",
    "    cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_default.xml\")\n",
    "    faces = cascade.detectMultiScale(image, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "    if not len(faces) > 0:\n",
    "        return [0]\n",
    "\n",
    "    # initialize the first face as having maximum area, then find the one with max_area\n",
    "    max_area_face = faces[0]\n",
    "    for face in faces:\n",
    "        if face[2] * face[3] > max_area_face[2] * max_area_face[3]:\n",
    "            max_area_face = face\n",
    "    face = max_area_face\n",
    "\n",
    "    # extract ROI of face\n",
    "    image = image[face[1]:(face[1] + face[2]), face[0]:(face[0] + face[3])]\n",
    "\n",
    "    try:\n",
    "        # resize the image so that it can be passed to the neural network\n",
    "        image = cv2.resize(\n",
    "            image, (48, 48), interpolation=cv2.INTER_CUBIC) / 255.\n",
    "    except Exception:\n",
    "        print(\"----->Problem during resize\")\n",
    "        return None\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/angry/*\n",
      "125\n",
      "./images/happy/*\n",
      "175\n",
      "./images/neutral/*\n",
      "130\n",
      "./images/sad/*\n",
      "125\n",
      "./images/scared/*\n",
      "112\n"
     ]
    }
   ],
   "source": [
    "training_data, training_labels = make_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with tf.Graph().as_default():\n",
    "#     g = tflearn.input_data(shape=[None, 48, 48, 1])\n",
    "#     print(\"Input data     \", g.shape[1:])\n",
    "    \n",
    "#     g = tflearn.conv_2d(g, 64, 5, activation='relu')\n",
    "#     print(\"Conv1          \", g.shape[1:])\n",
    "    \n",
    "#     g = tflearn.max_pool_2d(g, 3, strides=2)\n",
    "#     print(\"Maxpool1       \", g.shape[1:])\n",
    "    \n",
    "#     g = tflearn.conv_2d(g, 64, 5, activation='relu')\n",
    "#     print(\"Conv2          \", g.shape[1:])\n",
    "    \n",
    "#     g = tflearn.max_pool_2d(g, 3, strides=2)\n",
    "#     print(\"Maxpool2       \", g.shape[1:])\n",
    "    \n",
    "#     g = tflearn.conv_2d(g, 128, 4, activation='relu')\n",
    "#     print(\"Conv3          \", g.shape[1:])\n",
    "    \n",
    "#     g = tflearn.dropout(g, 0.3)\n",
    "#     print(\"Dropout        \", g.shape[1:])\n",
    "    \n",
    "\n",
    "#     g = tflearn.fully_connected(g, 3072, activation='relu')\n",
    "#     print(\"Fully connected\", g.shape[1:])\n",
    "    \n",
    "#     g = tflearn.fully_connected(g, len(target_classes), activation='softmax')\n",
    "#     print(\"Output         \", g.shape[1:])\n",
    "    \n",
    "#     g = tflearn.regression(g, optimizer='momentum',\n",
    "#                      metric='accuracy', loss='categorical_crossentropy')\n",
    "\n",
    "#     m = tflearn.DNN(g)\n",
    "    \n",
    "#     m.fit(training_data, training_labels)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data      (48, 48, 1)\n",
      "Conv1           (48, 48, 64)\n",
      "Maxpool1        (24, 24, 64)\n",
      "Conv2           (24, 24, 64)\n",
      "Maxpool2        (12, 12, 64)\n",
      "Conv3           (12, 12, 128)\n",
      "Dropout         (12, 12, 128)\n",
      "Fully connected (3072,)\n",
      "Output          (5,)\n"
     ]
    }
   ],
   "source": [
    "network = tflearn.input_data(shape=[None, 48, 48,1])\n",
    "print(\"Input data     \", network.shape[1:])\n",
    "\n",
    "network = conv_2d(network, 64, 5, activation='relu')\n",
    "print(\"Conv1          \", network.shape[1:])\n",
    "\n",
    "network = max_pool_2d(network, 3, strides=2)\n",
    "print(\"Maxpool1       \", network.shape[1:])\n",
    "\n",
    "network = conv_2d(network, 64, 5, activation='relu')\n",
    "print(\"Conv2          \", network.shape[1:])\n",
    "\n",
    "network = max_pool_2d(network, 3, strides=2)\n",
    "print(\"Maxpool2       \", network.shape[1:])\n",
    "\n",
    "network = conv_2d(network, 128, 4, activation='relu')\n",
    "print(\"Conv3          \", network.shape[1:])\n",
    "\n",
    "network = dropout(network, 0.3)\n",
    "print(\"Dropout        \", network.shape[1:])\n",
    "\n",
    "network = fully_connected(network, 3072, activation='relu')\n",
    "print(\"Fully connected\", network.shape[1:])\n",
    "\n",
    "network = fully_connected(network, len(target_classes), activation='softmax')\n",
    "print(\"Output         \", network.shape[1:])\n",
    "# Generates a TrainOp which contains the information about optimization process - optimizer, loss function, etc\n",
    "\n",
    "# network = fully_connected(network, 1, activation='softmax')\n",
    "# print(\"Output         \", network.shape[1:])\n",
    "# # Generates a TrainOp which contains the information about optimization process - optimizer, loss function, etc\n",
    "\n",
    "\n",
    "network = regression(network, optimizer='momentum',\n",
    "                     metric='accuracy', loss='categorical_crossentropy')\n",
    "\n",
    "# Creates a model instance.\n",
    "model = tflearn.DNN(network, checkpoint_path='model_1_atul',\n",
    "                    max_checkpoints=1, tensorboard_verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-0a4494a81c4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tflearn/models/dnn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X_inputs, Y_targets, n_epoch, validation_set, show_metric, batch_size, shuffle, snapshot_epoch, snapshot_step, excl_trainops, validation_batch_size, run_id, callbacks)\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# TODO: check memory impact for large data and multiple optimizers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         feed_dict = feed_dict_builder(X_inputs, Y_targets, self.inputs,\n\u001b[0;32m--> 184\u001b[0;31m                                       self.targets)\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mfeed_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mval_feed_dicts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/anaconda3/lib/python3.6/site-packages/tflearn/utils.py\u001b[0m in \u001b[0;36mfeed_dict_builder\u001b[0;34m(X, Y, net_inputs, net_targets)\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 300\u001b[0;31m                 \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnet_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    301\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m             \u001b[0;31m# If a dict is provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.fit(training_data, training_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_data.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
